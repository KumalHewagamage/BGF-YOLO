# Improved YOLOv8 configuration with accuracy enhancements
# Based on YOLOv8 with architectural improvements for better detection performance

# Parameters
nc: 1  # number of classes
scales: # model compound scaling constants
  # [depth, width, max_channels]
  n: [0.33, 0.25, 1024]
  s: [0.33, 0.50, 1024]
  m: [0.67, 0.75, 768]
  l: [1.00, 1.00, 512]
  x: [1.00, 1.25, 512]

# IMPROVED backbone - Added attention and more feature extraction
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]      # 0-P1/2
  - [-1, 1, Conv, [128, 3, 2]]     # 1-P2/4
  - [-1, 3, C2f, [128, True]]      # 2
  - [-1, 1, eca_block, [128]]      # 3 - ADD: ECA attention for early features
  
  - [-1, 1, Conv, [256, 3, 2]]     # 4-P3/8
  - [-1, 9, C2f, [256, True]]      # 5 - IMPROVED: 9 repeats (was 6) for richer P3 features
  - [-1, 1, eca_block, [256]]      # 6 - ADD: ECA attention
  
  - [-1, 1, Conv, [512, 3, 2]]     # 7-P4/16
  - [-1, 9, C2f, [512, True]]      # 8 - IMPROVED: 9 repeats (was 6)
  - [-1, 1, eca_block, [512]]      # 9 - ADD: ECA attention
  
  - [-1, 1, Conv, [1024, 3, 2]]    # 10-P5/32
  - [-1, 6, C2f, [1024, True]]     # 11 - IMPROVED: 6 repeats (was 3) for deeper features
  - [-1, 1, SPPF, [1024, 5]]       # 12 - Spatial Pyramid Pooling

# IMPROVED head - Bidirectional FPN with attention mechanisms
head:
  # Top-down pathway with attention
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]     # 13
  - [-1, 1, BiLevelRoutingAttention, [8]]          # 14 - IMPROVED: Better attention than CBAM
  - [[-1, 8], 1, Concat, [1]]                      # 15 - cat backbone P4
  - [-1, 6, C2f, [512]]                            # 16 - IMPROVED: 6 repeats (was 3)
  
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]     # 17
  - [-1, 1, BiLevelRoutingAttention, [8]]          # 18 - ADD: Attention layer
  - [[-1, 5], 1, Concat, [1]]                      # 19 - cat backbone P3
  - [-1, 6, C2f, [256]]                            # 20 - IMPROVED: 6 repeats (was 3) - P3/8
  
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]     # 21
  - [-1, 1, BiLevelRoutingAttention, [8]]          # 22 - ADD: Attention layer
  - [[-1, 3], 1, Concat, [1]]                      # 23 - cat backbone P2
  - [-1, 3, C2f, [128]]                            # 24 - P2/4 for small objects

  # Bottom-up pathway with cross-scale connections
  - [-1, 1, Conv, [128, 3, 2]]                     # 25
  - [-1, 1, eca_block, [128]]                      # 26 - ADD: Lightweight attention
  - [[-1, 20], 1, Concat, [1]]                     # 27 - cat with P3 features
  - [-1, 6, C2f, [256]]                            # 28 - IMPROVED: 6 repeats - P3/8
  
  - [-1, 1, Conv, [256, 3, 2]]                     # 29
  - [-1, 1, eca_block, [256]]                      # 30 - ADD: Lightweight attention
  - [[-1, 16], 1, Concat, [1]]                     # 31 - cat with P4 features
  - [-1, 6, C2f, [512]]                            # 32 - IMPROVED: 6 repeats - P4/16
  
  - [-1, 1, Conv, [512, 3, 2]]                     # 33
  - [-1, 1, eca_block, [512]]                      # 34 - ADD: Lightweight attention
  - [[-1, 12], 1, Concat, [1]]                     # 35 - cat with backbone P5
  - [-1, 3, C2f, [1024]]                           # 36 - P5/32

  # Multi-scale detection heads
  - [[24, 28, 32, 36], 1, Detect, [nc]]            # 37 - Detect(P2, P3, P4, P5) - 4 scales!


# KEY IMPROVEMENTS MADE:
# 1. ✅ Added ECA attention blocks in backbone (lightweight, effective)
# 2. ✅ Increased C2f repeats in critical layers (deeper feature extraction)
# 3. ✅ Replaced CBAM with BiLevelRoutingAttention in head (better long-range modeling)
# 4. ✅ Added P2 detection head (160×160) for SMALL object detection
# 5. ✅ Increased head C2f repeats from 3 to 6 (richer features)
# 6. ✅ Added ECA blocks in bottom-up pathway (efficient attention)
# 7. ✅ 4-scale detection instead of 3 (better multi-scale coverage)

# EXPECTED IMPROVEMENTS:
# - Better small object detection (P2 head)
# - Improved feature representation (more C2f repeats)
# - Better feature refinement (attention mechanisms)
# - Enhanced multi-scale fusion (4 detection heads)
# - Minimal computational overhead (ECA is lightweight)

# TRADE-OFFS:
# - Slightly slower training/inference (~15-20% slower)
# - More parameters (~25-30% increase)
# - Higher GPU memory usage
# - But significantly better accuracy potential
